{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Кластеризация с фильтрацией POS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11314, 7532)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "\n",
    "\n",
    "train_data = fetch_20newsgroups(subset='train')\n",
    "val_data = fetch_20newsgroups(subset='test')\n",
    "\n",
    "len(train_data['data']), len(val_data['data'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install -qq spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package words to /Users/ktann/nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "import re\n",
    "from collections import Counter\n",
    "from spacy.lang.en import stop_words\n",
    "\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import words\n",
    "nltk.download('words')\n",
    "\n",
    "\n",
    "DICT_WORDS_COUNT = 1000\n",
    "stopwords = stop_words.STOP_WORDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "236736"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "english_words_corpora = words.words()\n",
    "len(english_words_corpora)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tag import pos_tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "\n",
    "    if 'Lines:' in text:\n",
    "        start = text.index('Lines:') + 5\n",
    "        text = text[start:]\n",
    "\n",
    "    text = text.lower()\n",
    "\n",
    "    text = re.sub(r'[\\w\\d.]+@[\\w\\d]+.[\\w\\d]+', ' ', text)\n",
    "    text = re.sub(r'(http|https)://[\\w\\d/.]+', ' ', text)\n",
    "\n",
    "    text = re.sub(r'[^a-z]', ' ', text)\n",
    "\n",
    "    text = re.sub(r'([^\\w]|[-_])+', ' ', text)\n",
    "    text = re.sub(r'\\d+', ' ', text)\n",
    "    text = re.sub(r'(.)\\1{2,}', r'\\1', text)\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "\n",
    "    text = text.strip()\n",
    "    \n",
    "    words = [\n",
    "        w for w in text.split() \\\n",
    "            if len(w) > 3 and \\\n",
    "                w not in stopwords and \\\n",
    "                w in english_words_corpora\n",
    "    ]\n",
    "\n",
    "    return ' '.join(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def delete_pos_tags(text, acceptable_poses: tuple | list):\n",
    "    tokens = word_tokenize(text, language='english')\n",
    "    tags = pos_tag(tokens, lang='eng')\n",
    "    _, pos_tags = zip(*tags)\n",
    "\n",
    "    pairs = zip(text.split(), pos_tags)\n",
    "    pairs = filter(lambda x: x[1] not in acceptable_poses, pairs)\n",
    "    words = [p[0] for p in pairs]\n",
    "\n",
    "    return ' '.join(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "ACCEPTABLE_POS_TAGS = [\n",
    "    'NOUN',\n",
    "    'ADJ',\n",
    "    'VERB',\n",
    "    'NUM',\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import randint, seed\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed(4242)\n",
    "samples1000 = list(filter(lambda it: randint(0, 9) == 0, zip(train_data['data'], train_data['target'])))\n",
    "# samples1000 = list(zip(train_data['data'], train_data['target']))\n",
    "texts, targets = zip(*samples1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train texts:   0%|          | 0/1131 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train texts: 100%|██████████| 1131/1131 [01:38<00:00, 11.51it/s]\n"
     ]
    }
   ],
   "source": [
    "clean_train_texts = [\n",
    "    clean_text(text) for text in tqdm(texts, desc='Train texts')\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Delete pos tags: 100%|██████████| 1131/1131 [00:01<00:00, 640.52it/s]\n"
     ]
    }
   ],
   "source": [
    "# NOUNS\n",
    "clean_train_texts1 = [\n",
    "    delete_pos_tags(text, ACCEPTABLE_POS_TAGS[:1])\n",
    "    for text in tqdm(clean_train_texts, desc='Delete pos tags')\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Delete pos tags: 100%|██████████| 1131/1131 [00:01<00:00, 648.85it/s]\n"
     ]
    }
   ],
   "source": [
    "# NOUNS, ADJ\n",
    "clean_train_texts2 = [\n",
    "    delete_pos_tags(text, ACCEPTABLE_POS_TAGS[:2])\n",
    "    for text in tqdm(clean_train_texts, desc='Delete pos tags')\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Delete pos tags: 100%|██████████| 1131/1131 [00:01<00:00, 636.59it/s]\n"
     ]
    }
   ],
   "source": [
    "# NOUNS, ADJ, VERB\n",
    "clean_train_texts3 = [\n",
    "    delete_pos_tags(text, ACCEPTABLE_POS_TAGS[:3])\n",
    "    for text in tqdm(clean_train_texts, desc='Delete pos tags')\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Delete pos tags: 100%|██████████| 1131/1131 [00:01<00:00, 592.11it/s]\n"
     ]
    }
   ],
   "source": [
    "# NOUNS, ADJ, VERB, NUM\n",
    "clean_train_texts4 = [\n",
    "    delete_pos_tags(text, ACCEPTABLE_POS_TAGS[:4])\n",
    "    for text in tqdm(clean_train_texts, desc='Delete pos tags')\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'plus finally gave ghost weekend starting life market machine sooner intended looking maybe bunch hopefully somebody answer anybody know dirt round supposed summer haven access wondering anybody anybody price line like went recently impression display probably swing disk feel better display great store good solicit people worth taking disk size money active display realize real subjective question computer store figured somebody actually machine daily prove helpful perform thanks bunch advance post summary news reading time premium corner electrical engineering dangerous truth'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_train_texts1[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train KMeans, DBSCAN, Agglomerative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import DBSCAN, KMeans, AgglomerativeClustering\n",
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score, accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_kmeans_results(df):\n",
    "    dff = df.dropna()\n",
    "\n",
    "    clusterisator = KMeans(n_clusters=20, random_state=42)\n",
    "    dff['cluster'] = clusterisator.fit_predict(dff.drop('target', axis=1))\n",
    "\n",
    "    res = silhouette_score(dff.drop(['target', 'cluster'], axis=1), dff['cluster'])\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dbscan_results(df):\n",
    "    dff = df.dropna()\n",
    "\n",
    "    clusterisator = DBSCAN(eps=0.055, min_samples=2)\n",
    "    dff['cluster'] = clusterisator.fit_predict(dff.drop('target', axis=1))\n",
    "\n",
    "    res = silhouette_score(dff.drop(['target', 'cluster'], axis=1), dff['cluster'])\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_agglo_results(df):\n",
    "    dff = df.dropna()\n",
    "\n",
    "    clusterisator = AgglomerativeClustering(n_clusters=20)\n",
    "    dff['cluster'] = clusterisator.fit_predict(dff.drop('target', axis=1))\n",
    "\n",
    "    res = silhouette_score(dff.drop(['target', 'cluster'], axis=1), dff['cluster'])\n",
    "    return res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BOW, TF-IDF, LSI, LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from gensim import corpora, models\n",
    "from gensim.utils import simple_preprocess\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_bow(clean_train_texts, th=300):\n",
    "    tokenized_documents = [simple_preprocess(text) for text in clean_train_texts]\n",
    "    dictionary = corpora.Dictionary(tokenized_documents)\n",
    "    bow_corpus = [dictionary.doc2bow(doc) for doc in tokenized_documents]\n",
    "\n",
    "    df = pd.DataFrame(targets, columns=['target'])\n",
    "    \n",
    "    for i in range(th):\n",
    "        df[f'{i}'] = 0\n",
    "    \n",
    "    for i, b in enumerate(bow_corpus):\n",
    "        for (idx, count) in b:\n",
    "            if idx < th:\n",
    "                df.loc[i, f'{idx}'] = count\n",
    "\n",
    "    kmeans_res = get_kmeans_results(df)\n",
    "    dbscan_res = get_dbscan_results(df)\n",
    "    agglo_res = get_agglo_results(df)\n",
    "    \n",
    "    return kmeans_res, dbscan_res, agglo_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_tfidf(clean_train_texts, th=300):\n",
    "    tokenized_documents = [simple_preprocess(text) for text in clean_train_texts]\n",
    "    dictionary = corpora.Dictionary(tokenized_documents)\n",
    "    bow_corpus = [dictionary.doc2bow(doc) for doc in tokenized_documents]\n",
    "\n",
    "    tfidf = models.TfidfModel(bow_corpus)\n",
    "    corpus_tfidf = tfidf[bow_corpus]\n",
    "\n",
    "    df = pd.DataFrame(targets, columns=['target'])\n",
    "    \n",
    "    for i in range(th):\n",
    "        df[f'{i}'] = 0\n",
    "    \n",
    "    for i, b in enumerate(corpus_tfidf):\n",
    "        for (idx, val) in b:\n",
    "            if idx < th:\n",
    "                df.loc[i, f'{idx}'] = val\n",
    "\n",
    "    kmeans_res = get_kmeans_results(df)\n",
    "    dbscan_res = get_dbscan_results(df)\n",
    "    agglo_res = get_agglo_results(df)\n",
    "    \n",
    "    return kmeans_res, dbscan_res, agglo_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_lsi(clean_train_texts):\n",
    "    tokenized_documents = [simple_preprocess(text) for text in clean_train_texts]\n",
    "    dictionary = corpora.Dictionary(tokenized_documents)\n",
    "    bow_corpus = [dictionary.doc2bow(doc) for doc in tokenized_documents]\n",
    "\n",
    "    tfidf = models.TfidfModel(bow_corpus)\n",
    "    corpus_tfidf = tfidf[bow_corpus]\n",
    "\n",
    "    lsi_model = models.LsiModel(corpus_tfidf, id2word=dictionary, num_topics=20)\n",
    "\n",
    "    document_topic_vectors = []\n",
    "    for doc_bow in corpus_tfidf:\n",
    "        document_topic_vector = lsi_model[doc_bow]\n",
    "        document_topic_vectors.append(document_topic_vector)\n",
    "    \n",
    "    df = pd.DataFrame(targets, columns=['target'])\n",
    "    \n",
    "    docs_vectors = []\n",
    "\n",
    "    for doc_idx in tqdm(range(len(df))):\n",
    "        doc_bow = corpus_tfidf[doc_idx]\n",
    "        document_topic_vector = lsi_model[doc_bow]\n",
    "\n",
    "        if document_topic_vector:\n",
    "            _, vec = zip(*document_topic_vector)\n",
    "        else:\n",
    "            vec = [None] * 20\n",
    "\n",
    "        docs_vectors.append(vec)\n",
    "    \n",
    "    df[[f'vec{i}' for i in range(20)]] = docs_vectors\n",
    "\n",
    "    kmeans_res = get_kmeans_results(df)\n",
    "    dbscan_res = get_dbscan_results(df)\n",
    "    agglo_res = get_agglo_results(df)\n",
    "    \n",
    "    return kmeans_res, dbscan_res, agglo_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_lda(clean_train_texts):\n",
    "    tokenized_documents = [simple_preprocess(text) for text in clean_train_texts]\n",
    "    dictionary = corpora.Dictionary(tokenized_documents)\n",
    "    bow_corpus = [dictionary.doc2bow(doc) for doc in tokenized_documents]\n",
    "\n",
    "    tfidf = models.TfidfModel(bow_corpus)\n",
    "    corpus_tfidf = tfidf[bow_corpus]\n",
    "\n",
    "    lda_model = models.LdaModel(corpus_tfidf, num_topics=100, id2word=dictionary, passes=15)\n",
    "\n",
    "    document_topic_vectors = []\n",
    "\n",
    "    for i, doc_bow in enumerate(bow_corpus):\n",
    "        document_topics = lda_model.get_document_topics(doc_bow, minimum_probability=0.0)\n",
    "        document_topic_vector = [topic_prob for _, topic_prob in document_topics]\n",
    "        document_topic_vectors.append(document_topic_vector)\n",
    "    \n",
    "    df = pd.DataFrame(document_topic_vectors)\n",
    "    df['target'] = targets\n",
    "\n",
    "    kmeans_res = get_kmeans_results(df)\n",
    "    dbscan_res = get_dbscan_results(df)\n",
    "    agglo_res = get_agglo_results(df)\n",
    "    \n",
    "    return kmeans_res, dbscan_res, agglo_res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_texts = [\n",
    "    clean_train_texts1,\n",
    "    clean_train_texts2,\n",
    "    clean_train_texts3,\n",
    "    clean_train_texts4,\n",
    "]\n",
    "\n",
    "vectorizers = [\n",
    "    ('BOW', train_bow),\n",
    "    ('TF-IDF', train_tfidf),\n",
    "    ('LSI', train_lsi),\n",
    "    ('LDA', train_lda),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TAGS: ['NOUN']; vectorizer: BOW\n",
      "KMeans: 0.0837, DBSCAN: -0.3611, Agglo: 0.2898\n",
      "\n",
      "TAGS: ['NOUN']; vectorizer: TF-IDF\n",
      "KMeans: 0.0432, DBSCAN: -0.2949, Agglo: 0.0913\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1131/1131 [00:00<00:00, 8263.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TAGS: ['NOUN']; vectorizer: LSI\n",
      "KMeans: 0.1569, DBSCAN: -0.3824, Agglo: 0.1098\n",
      "\n",
      "TAGS: ['NOUN']; vectorizer: LDA\n",
      "KMeans: 0.0499, DBSCAN: -0.2225, Agglo: 0.0184\n",
      "\n",
      "TAGS: ['NOUN', 'ADJ']; vectorizer: BOW\n",
      "KMeans: 0.0837, DBSCAN: -0.3611, Agglo: 0.2898\n",
      "\n",
      "TAGS: ['NOUN', 'ADJ']; vectorizer: TF-IDF\n",
      "KMeans: 0.0432, DBSCAN: -0.2949, Agglo: 0.0913\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1131/1131 [00:00<00:00, 6500.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TAGS: ['NOUN', 'ADJ']; vectorizer: LSI\n",
      "KMeans: 0.1158, DBSCAN: -0.3808, Agglo: 0.0942\n",
      "\n",
      "TAGS: ['NOUN', 'ADJ']; vectorizer: LDA\n",
      "KMeans: 0.0709, DBSCAN: -0.2174, Agglo: 0.0650\n",
      "\n",
      "TAGS: ['NOUN', 'ADJ', 'VERB']; vectorizer: BOW\n",
      "KMeans: 0.0837, DBSCAN: -0.3611, Agglo: 0.2898\n",
      "\n",
      "TAGS: ['NOUN', 'ADJ', 'VERB']; vectorizer: TF-IDF\n",
      "KMeans: 0.0432, DBSCAN: -0.2949, Agglo: 0.0913\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1131/1131 [00:00<00:00, 7728.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TAGS: ['NOUN', 'ADJ', 'VERB']; vectorizer: LSI\n",
      "KMeans: 0.1173, DBSCAN: -0.3711, Agglo: 0.1370\n",
      "\n",
      "TAGS: ['NOUN', 'ADJ', 'VERB']; vectorizer: LDA\n",
      "KMeans: 0.0782, DBSCAN: -0.2260, Agglo: 0.0522\n",
      "\n",
      "TAGS: ['NOUN', 'ADJ', 'VERB', 'NUM']; vectorizer: BOW\n",
      "KMeans: 0.0837, DBSCAN: -0.3611, Agglo: 0.2898\n",
      "\n",
      "TAGS: ['NOUN', 'ADJ', 'VERB', 'NUM']; vectorizer: TF-IDF\n",
      "KMeans: 0.0432, DBSCAN: -0.2949, Agglo: 0.0913\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1131/1131 [00:00<00:00, 8694.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TAGS: ['NOUN', 'ADJ', 'VERB', 'NUM']; vectorizer: LSI\n",
      "KMeans: 0.1240, DBSCAN: -0.3743, Agglo: 0.1143\n",
      "\n",
      "TAGS: ['NOUN', 'ADJ', 'VERB', 'NUM']; vectorizer: LDA\n",
      "KMeans: 0.0887, DBSCAN: -0.1963, Agglo: 0.0495\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i, ct in enumerate(clean_texts):\n",
    "    for v_name, v in vectorizers:\n",
    "        kmeans_res, dbscan_res, agglo_res = v(ct)\n",
    "        print(f'TAGS: {ACCEPTABLE_POS_TAGS[:i+1]}; vectorizer: {v_name}')\n",
    "        print(f'KMeans: {kmeans_res:.4f}, DBSCAN: {dbscan_res:.4f}, Agglo: {agglo_res:.4f}')\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
