{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Задание практика №4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11314, 7532)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "\n",
    "\n",
    "train_data = fetch_20newsgroups(subset='train')\n",
    "val_data = fetch_20newsgroups(subset='test')\n",
    "\n",
    "len(train_data['data']), len(val_data['data'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install -qq spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Error loading words: <urlopen error [Errno 60] Operation\n",
      "[nltk_data]     timed out>\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "import re\n",
    "from collections import Counter\n",
    "from spacy.lang.en import stop_words\n",
    "\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import words\n",
    "nltk.download('words')\n",
    "\n",
    "\n",
    "DICT_WORDS_COUNT = 1000\n",
    "stopwords = stop_words.STOP_WORDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "236736"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "english_words_corpora = words.words()\n",
    "len(english_words_corpora)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tag import pos_tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "\n",
    "    if 'Lines:' in text:\n",
    "        start = text.index('Lines:') + 5\n",
    "        text = text[start:]\n",
    "\n",
    "    text = text.lower()\n",
    "\n",
    "    text = re.sub(r'[\\w\\d.]+@[\\w\\d]+.[\\w\\d]+', ' ', text)\n",
    "    text = re.sub(r'(http|https)://[\\w\\d/.]+', ' ', text)\n",
    "\n",
    "    text = re.sub(r'[^a-z]', ' ', text)\n",
    "\n",
    "    text = re.sub(r'([^\\w]|[-_])+', ' ', text)\n",
    "    text = re.sub(r'\\d+', ' ', text)\n",
    "    text = re.sub(r'(.)\\1{2,}', r'\\1', text)\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "\n",
    "    text = text.strip()\n",
    "    \n",
    "    words = [\n",
    "        w for w in text.split() \\\n",
    "            if len(w) > 3 and \\\n",
    "                w not in stopwords and \\\n",
    "                w in english_words_corpora\n",
    "    ]\n",
    "\n",
    "    return ' '.join(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def delete_pos_tags(text, acceptable_poses: tuple | list):\n",
    "    tokens = word_tokenize(text, language='english')\n",
    "    tags = pos_tag(tokens, lang='eng')\n",
    "    _, pos_tags = zip(*tags)\n",
    "\n",
    "    pairs = zip(text.split(), pos_tags)\n",
    "    pairs = filter(lambda x: x[1] not in acceptable_poses, pairs)\n",
    "    words = [p[0] for p in pairs]\n",
    "\n",
    "    return ' '.join(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "ACCEPTABLE_POS_TAGS = [\n",
    "    'NOUN',\n",
    "    'ADJ',\n",
    "    'VERB',\n",
    "    'NUM',\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import randint, seed\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed(4242)\n",
    "samples1000 = list(filter(lambda it: randint(0, 9) == 0, zip(train_data['data'], train_data['target'])))\n",
    "# samples1000 = list(zip(train_data['data'], train_data['target']))\n",
    "texts, targets = zip(*samples1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train texts: 100%|██████████| 1131/1131 [01:36<00:00, 11.70it/s]\n"
     ]
    }
   ],
   "source": [
    "clean_train_texts = [\n",
    "    clean_text(text) for text in tqdm(texts, desc='Train texts')\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Delete pos tags: 100%|██████████| 1131/1131 [00:01<00:00, 618.15it/s]\n"
     ]
    }
   ],
   "source": [
    "# NOUNS\n",
    "clean_train_texts1 = [\n",
    "    delete_pos_tags(text, ACCEPTABLE_POS_TAGS[:1])\n",
    "    for text in tqdm(clean_train_texts, desc='Delete pos tags')\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Delete pos tags:   0%|          | 0/1131 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Delete pos tags: 100%|██████████| 1131/1131 [00:01<00:00, 629.70it/s]\n"
     ]
    }
   ],
   "source": [
    "# NOUNS, ADJ\n",
    "clean_train_texts2 = [\n",
    "    delete_pos_tags(text, ACCEPTABLE_POS_TAGS[:2])\n",
    "    for text in tqdm(clean_train_texts, desc='Delete pos tags')\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Delete pos tags: 100%|██████████| 1131/1131 [00:01<00:00, 620.23it/s]\n"
     ]
    }
   ],
   "source": [
    "# NOUNS, ADJ, VERB\n",
    "clean_train_texts3 = [\n",
    "    delete_pos_tags(text, ACCEPTABLE_POS_TAGS[:3])\n",
    "    for text in tqdm(clean_train_texts, desc='Delete pos tags')\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Delete pos tags: 100%|██████████| 1131/1131 [00:01<00:00, 625.79it/s]\n"
     ]
    }
   ],
   "source": [
    "# NOUNS, ADJ, VERB, NUM\n",
    "clean_train_texts4 = [\n",
    "    delete_pos_tags(text, ACCEPTABLE_POS_TAGS[:4])\n",
    "    for text in tqdm(clean_train_texts, desc='Delete pos tags')\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'plus finally gave ghost weekend starting life market machine sooner intended looking maybe bunch hopefully somebody answer anybody know dirt round supposed summer haven access wondering anybody anybody price line like went recently impression display probably swing disk feel better display great store good solicit people worth taking disk size money active display realize real subjective question computer store figured somebody actually machine daily prove helpful perform thanks bunch advance post summary news reading time premium corner electrical engineering dangerous truth'"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_train_texts1[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train CatBoost, RF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install -qq catboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from catboost import CatBoostClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score, accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_catboost_results(df):\n",
    "    X_train, X_val, y_train, y_val = train_test_split(\n",
    "        df.drop('target', axis=1),\n",
    "        df['target'],\n",
    "        test_size=0.2,\n",
    "        shuffle=True,\n",
    "        random_state=42,\n",
    "    )\n",
    "                \n",
    "    clf = CatBoostClassifier(\n",
    "        iterations=2500,\n",
    "        # verbose=200,\n",
    "        logging_level='Silent',\n",
    "    )\n",
    "    clf.fit(\n",
    "        X=X_train,\n",
    "        y=y_train,\n",
    "        eval_set=(X_val, y_val),\n",
    "        early_stopping_rounds=50,\n",
    "    )\n",
    "\n",
    "    y_pred = clf.predict(X_val)\n",
    "    acc = accuracy_score(y_val, y_pred)\n",
    "    f1 = f1_score(y_val, y_pred, average='macro')\n",
    "\n",
    "    return acc, f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_rf_results(df):\n",
    "    best_acc, best_f1 = 0, 0\n",
    "\n",
    "    for md in [8, 16, 32, 64]:\n",
    "        md = int(md)\n",
    "\n",
    "        dff = df.dropna()\n",
    "\n",
    "        X_train, X_val, y_train, y_val = train_test_split(\n",
    "            dff.drop('target', axis=1),\n",
    "            dff['target'],\n",
    "            test_size=0.2,\n",
    "            shuffle=True,\n",
    "            random_state=42,\n",
    "        )\n",
    "                    \n",
    "        clf = RandomForestClassifier(\n",
    "            max_depth=md,\n",
    "            random_state=42,\n",
    "        )\n",
    "        clf.fit(X_train, y_train)\n",
    "\n",
    "        y_pred = clf.predict(X_val)\n",
    "        acc = accuracy_score(y_val, y_pred)\n",
    "        f1 = f1_score(y_val, y_pred, average='macro')\n",
    "\n",
    "        best_acc = max(best_acc, acc)\n",
    "        best_f1 = max(best_acc, f1)\n",
    "    \n",
    "    return best_acc, best_f1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BOW, TF-IDF, LSI, LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from gensim import corpora, models\n",
    "from gensim.utils import simple_preprocess\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_bow(clean_train_texts, th=300):\n",
    "    tokenized_documents = [simple_preprocess(text) for text in clean_train_texts]\n",
    "    dictionary = corpora.Dictionary(tokenized_documents)\n",
    "    bow_corpus = [dictionary.doc2bow(doc) for doc in tokenized_documents]\n",
    "\n",
    "    df = pd.DataFrame(targets, columns=['target'])\n",
    "    \n",
    "    for i in range(th):\n",
    "        df[f'{i}'] = 0\n",
    "    \n",
    "    for i, b in enumerate(bow_corpus):\n",
    "        for (idx, count) in b:\n",
    "            if idx < th:\n",
    "                df.loc[i, f'{idx}'] = count\n",
    "\n",
    "    # train catboost & RF\n",
    "    cb_acc, cb_f1 = get_catboost_results(df)\n",
    "    rf_acc, rf_f1 = get_rf_results(df)\n",
    "    \n",
    "    return cb_acc, cb_f1, rf_acc, rf_f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_tfidf(clean_train_texts, th=300):\n",
    "    tokenized_documents = [simple_preprocess(text) for text in clean_train_texts]\n",
    "    dictionary = corpora.Dictionary(tokenized_documents)\n",
    "    bow_corpus = [dictionary.doc2bow(doc) for doc in tokenized_documents]\n",
    "\n",
    "    tfidf = models.TfidfModel(bow_corpus)\n",
    "    corpus_tfidf = tfidf[bow_corpus]\n",
    "\n",
    "    df = pd.DataFrame(targets, columns=['target'])\n",
    "    \n",
    "    for i in range(th):\n",
    "        df[f'{i}'] = 0\n",
    "    \n",
    "    for i, b in enumerate(corpus_tfidf):\n",
    "        for (idx, val) in b:\n",
    "            if idx < th:\n",
    "                df.loc[i, f'{idx}'] = val\n",
    "\n",
    "    # train catboost & RF\n",
    "    cb_acc, cb_f1 = get_catboost_results(df)\n",
    "    rf_acc, rf_f1 = get_rf_results(df)\n",
    "    \n",
    "    return cb_acc, cb_f1, rf_acc, rf_f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_lsi(clean_train_texts):\n",
    "    tokenized_documents = [simple_preprocess(text) for text in clean_train_texts]\n",
    "    dictionary = corpora.Dictionary(tokenized_documents)\n",
    "    bow_corpus = [dictionary.doc2bow(doc) for doc in tokenized_documents]\n",
    "\n",
    "    tfidf = models.TfidfModel(bow_corpus)\n",
    "    corpus_tfidf = tfidf[bow_corpus]\n",
    "\n",
    "    lsi_model = models.LsiModel(corpus_tfidf, id2word=dictionary, num_topics=20)\n",
    "\n",
    "    document_topic_vectors = []\n",
    "    for doc_bow in corpus_tfidf:\n",
    "        document_topic_vector = lsi_model[doc_bow]\n",
    "        document_topic_vectors.append(document_topic_vector)\n",
    "    \n",
    "    df = pd.DataFrame(targets, columns=['target'])\n",
    "    \n",
    "    docs_vectors = []\n",
    "\n",
    "    for doc_idx in tqdm(range(len(df))):\n",
    "        doc_bow = corpus_tfidf[doc_idx]\n",
    "        document_topic_vector = lsi_model[doc_bow]\n",
    "\n",
    "        if document_topic_vector:\n",
    "            _, vec = zip(*document_topic_vector)\n",
    "        else:\n",
    "            vec = [None] * 20\n",
    "\n",
    "        docs_vectors.append(vec)\n",
    "    \n",
    "    df[[f'vec{i}' for i in range(20)]] = docs_vectors\n",
    "\n",
    "    # train catboost & RF\n",
    "    cb_acc, cb_f1 = get_catboost_results(df)\n",
    "    rf_acc, rf_f1 = get_rf_results(df)\n",
    "    \n",
    "    return cb_acc, cb_f1, rf_acc, rf_f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_lda(clean_train_texts):\n",
    "    tokenized_documents = [simple_preprocess(text) for text in clean_train_texts]\n",
    "    dictionary = corpora.Dictionary(tokenized_documents)\n",
    "    bow_corpus = [dictionary.doc2bow(doc) for doc in tokenized_documents]\n",
    "\n",
    "    tfidf = models.TfidfModel(bow_corpus)\n",
    "    corpus_tfidf = tfidf[bow_corpus]\n",
    "\n",
    "    lda_model = models.LdaModel(corpus_tfidf, num_topics=100, id2word=dictionary, passes=15)\n",
    "\n",
    "    document_topic_vectors = []\n",
    "\n",
    "    for i, doc_bow in enumerate(bow_corpus):\n",
    "        document_topics = lda_model.get_document_topics(doc_bow, minimum_probability=0.0)\n",
    "        document_topic_vector = [topic_prob for _, topic_prob in document_topics]\n",
    "        document_topic_vectors.append(document_topic_vector)\n",
    "    \n",
    "    df = pd.DataFrame(document_topic_vectors)\n",
    "    df['target'] = targets\n",
    "\n",
    "    # train catboost & RF\n",
    "    cb_acc, cb_f1 = get_catboost_results(df)\n",
    "    rf_acc, rf_f1 = get_rf_results(df)\n",
    "    \n",
    "    return cb_acc, cb_f1, rf_acc, rf_f1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_texts = [\n",
    "    clean_train_texts1,\n",
    "    clean_train_texts2,\n",
    "    clean_train_texts3,\n",
    "    clean_train_texts4,\n",
    "]\n",
    "\n",
    "vectorizers = [\n",
    "    ('BOW', train_bow),\n",
    "    ('TF-IDF', train_tfidf),\n",
    "    ('LSI', train_lsi),\n",
    "    ('LDA', train_lda),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TAGS: ['NOUN']; vectorizer: BOW\n",
      "RESULTS:\n",
      "CatBoost: acc=0.1982, f1=0.1708\n",
      "      RF: acc=0.2203, f1=0.2203\n",
      "\n",
      "TAGS: ['NOUN']; vectorizer: TF-IDF\n",
      "RESULTS:\n",
      "CatBoost: acc=0.1938, f1=0.1800\n",
      "      RF: acc=0.2291, f1=0.2291\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1131/1131 [00:00<00:00, 7454.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TAGS: ['NOUN']; vectorizer: LSI\n",
      "RESULTS:\n",
      "CatBoost: acc=0.5066, f1=0.4870\n",
      "      RF: acc=0.4802, f1=0.4802\n",
      "\n",
      "TAGS: ['NOUN']; vectorizer: LDA\n",
      "RESULTS:\n",
      "CatBoost: acc=0.3128, f1=0.2856\n",
      "      RF: acc=0.2643, f1=0.2643\n",
      "\n",
      "TAGS: ['NOUN', 'ADJ']; vectorizer: BOW\n",
      "RESULTS:\n",
      "CatBoost: acc=0.1982, f1=0.1708\n",
      "      RF: acc=0.2203, f1=0.2203\n",
      "\n",
      "TAGS: ['NOUN', 'ADJ']; vectorizer: TF-IDF\n",
      "RESULTS:\n",
      "CatBoost: acc=0.1938, f1=0.1800\n",
      "      RF: acc=0.2291, f1=0.2291\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1131/1131 [00:00<00:00, 8954.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TAGS: ['NOUN', 'ADJ']; vectorizer: LSI\n",
      "RESULTS:\n",
      "CatBoost: acc=0.4714, f1=0.4383\n",
      "      RF: acc=0.4581, f1=0.4581\n",
      "\n",
      "TAGS: ['NOUN', 'ADJ']; vectorizer: LDA\n",
      "RESULTS:\n",
      "CatBoost: acc=0.2247, f1=0.2009\n",
      "      RF: acc=0.2731, f1=0.2731\n",
      "\n",
      "TAGS: ['NOUN', 'ADJ', 'VERB']; vectorizer: BOW\n",
      "RESULTS:\n",
      "CatBoost: acc=0.1982, f1=0.1708\n",
      "      RF: acc=0.2203, f1=0.2203\n",
      "\n",
      "TAGS: ['NOUN', 'ADJ', 'VERB']; vectorizer: TF-IDF\n",
      "RESULTS:\n",
      "CatBoost: acc=0.1938, f1=0.1800\n",
      "      RF: acc=0.2291, f1=0.2291\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1131/1131 [00:00<00:00, 8596.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TAGS: ['NOUN', 'ADJ', 'VERB']; vectorizer: LSI\n",
      "RESULTS:\n",
      "CatBoost: acc=0.4934, f1=0.4602\n",
      "      RF: acc=0.5022, f1=0.5022\n",
      "\n",
      "TAGS: ['NOUN', 'ADJ', 'VERB']; vectorizer: LDA\n",
      "RESULTS:\n",
      "CatBoost: acc=0.2467, f1=0.2262\n",
      "      RF: acc=0.2203, f1=0.2203\n",
      "\n",
      "TAGS: ['NOUN', 'ADJ', 'VERB', 'NUM']; vectorizer: BOW\n",
      "RESULTS:\n",
      "CatBoost: acc=0.1982, f1=0.1708\n",
      "      RF: acc=0.2203, f1=0.2203\n",
      "\n",
      "TAGS: ['NOUN', 'ADJ', 'VERB', 'NUM']; vectorizer: TF-IDF\n",
      "RESULTS:\n",
      "CatBoost: acc=0.1938, f1=0.1800\n",
      "      RF: acc=0.2291, f1=0.2291\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1131/1131 [00:00<00:00, 7254.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TAGS: ['NOUN', 'ADJ', 'VERB', 'NUM']; vectorizer: LSI\n",
      "RESULTS:\n",
      "CatBoost: acc=0.4846, f1=0.4590\n",
      "      RF: acc=0.4890, f1=0.4890\n",
      "\n",
      "TAGS: ['NOUN', 'ADJ', 'VERB', 'NUM']; vectorizer: LDA\n",
      "RESULTS:\n",
      "CatBoost: acc=0.2555, f1=0.2190\n",
      "      RF: acc=0.2643, f1=0.2643\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i, ct in enumerate(clean_texts):\n",
    "    for v_name, v in vectorizers:\n",
    "        cb_acc, cb_f1, rf_acc, rf_f1 = v(ct)\n",
    "        print(f'TAGS: {ACCEPTABLE_POS_TAGS[:i+1]}; vectorizer: {v_name}')\n",
    "        print('RESULTS:')\n",
    "        print(f'CatBoost: acc={cb_acc:.4f}, f1={cb_f1:.4f}')\n",
    "        print(f'      RF: acc={rf_acc:.4f}, f1={rf_f1:.4f}')\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
